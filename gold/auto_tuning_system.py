#!/usr/bin/env python3
"""
ìë™í™”ëœ ê¸ˆ ê°€ê²© ì˜ˆì¸¡ ì‹œìŠ¤í…œ
- Optunaë¥¼ ì‚¬ìš©í•œ ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹
- ìƒê´€ê´€ê³„ ê¸°ë°˜ ì ì§„ì  íŠ¹ì„± ì„ íƒ
- MLflowë¥¼ í™œìš©í•œ ì‹¤í—˜ ì¶”ì 
- RÂ² > 0.5 ëª©í‘œ ë‹¬ì„± ìë™í™”
"""

import optuna
import mlflow
import mlflow.keras
import numpy as np
import pandas as pd
import json
import os
import shutil
import argparse
from datetime import datetime, timedelta
import warnings

# TensorFlow/macOS compatibility bootstrap BEFORE importing tensorflow or modules that import it
from tf_compat import bootstrap, configure_tensorflow
bootstrap()

from analysis.gold_data_manager import GoldDataManager
from analysis.gold_prediction_model import GoldPredictionModel
import tensorflow as tf
configure_tensorflow()

warnings.filterwarnings('ignore')

class AutoTuningSystem:
    def __init__(self, target_r2=0.5, max_trials=50):
        self.target_r2 = target_r2
        self.max_trials = max_trials
        self.best_r2 = -float('inf')
        self.best_model = None
        self.best_config = None
        # ê²½ë¡œ êµ¬ì„±
        self.base_dir = os.path.dirname(os.path.abspath(__file__))
        self.tmp_dir = os.path.join(self.base_dir, 'tmp', 'optuna')
        self.ensemble_dir = os.path.join(self.base_dir, 'models', 'ensemble')
        os.makedirs(self.tmp_dir, exist_ok=True)
        os.makedirs(self.ensemble_dir, exist_ok=True)
        
        # MLflow ì„¤ì •
        mlflow.set_tracking_uri(f"file://{self.base_dir}/mlruns")
        mlflow.set_experiment("gold_price_prediction")
        
        # ì¬í˜„ì„±ì„ ìœ„í•œ ì‹œë“œ ì„¤ì •
        self.set_random_seeds()
        
        print(f"ğŸš€ ìë™í™”ëœ ê¸ˆ ê°€ê²© ì˜ˆì¸¡ ì‹œìŠ¤í…œ ì‹œì‘")
        print(f"ğŸ¯ ëª©í‘œ RÂ²: {target_r2}")
        print(f"ğŸ”¬ ìµœëŒ€ ì‹¤í—˜ íšŸìˆ˜: {max_trials}")
    
    def set_random_seeds(self, seed=42):
        """ì¬í˜„ ê°€ëŠ¥í•œ ê²°ê³¼ë¥¼ ìœ„í•œ ì‹œë“œ ì„¤ì •"""
        np.random.seed(seed)
        tf.random.set_seed(seed)
        os.environ['PYTHONHASHSEED'] = str(seed)
    
    def get_progressive_features(self, data, max_features=10):
        """ìƒê´€ê´€ê³„ ê¸°ë°˜ ì ì§„ì  íŠ¹ì„± ì„ íƒ"""
        print("ğŸ” ì ì§„ì  íŠ¹ì„± ì„ íƒ ì‹œì‘...")
        
        # ì»¬ëŸ¼ì´ ë‹¤ì¤‘ ì¸ë±ìŠ¤ì´ë©´ í‰íƒ„í™”
        if isinstance(data.columns, pd.MultiIndex):
            try:
                data = data.copy()
                data.columns = [col[0] if isinstance(col, tuple) else col for col in data.columns]
            except Exception:
                # ì‹¤íŒ¨ ì‹œ ë¬¸ìì—´ ê²°í•©ìœ¼ë¡œ í‰íƒ„í™”
                data = data.copy()
                data.columns = ["_".join(map(str, col)) if isinstance(col, tuple) else str(col) for col in data.columns]

        # ëª©í‘œ ì»¬ëŸ¼ ì¡´ì¬ í™•ì¸
        # ê¸°ë³¸ íŠ¹ì„± (í•­ìƒ í¬í•¨)
        base_features = []
        if 'Close' in data.columns:
            base_features.append('Close')
        else:
            raise ValueError("í•„ìˆ˜ ì»¬ëŸ¼ 'Close' ì´(ê°€) ë°ì´í„°ì— ì—†ìŠµë‹ˆë‹¤.")
        if 'Volume' in data.columns:
            base_features.append('Volume')
        
        # ìˆ«ìí˜• ì»¬ëŸ¼ë§Œ í›„ë³´ë¡œ ì‚¬ìš©
        numeric_cols = [c for c in data.columns if pd.api.types.is_numeric_dtype(data[c])]
        available_features = [col for col in numeric_cols if col not in base_features]
        
        # Closeì™€ì˜ ìƒê´€ê´€ê³„ ê³„ì‚°
        correlations = data[available_features + ['Close']].corr()['Close'].abs().sort_values(ascending=False)
        
        # ìƒê´€ê´€ê³„ê°€ ë„ˆë¬´ ë†’ì§€ ì•Šì€ íŠ¹ì„±ë“¤ ì„ íƒ (ë‹¤ì¤‘ê³µì„ ì„± ë°©ì§€)
        selected_features = base_features.copy()
        correlation_threshold = 0.8
        
        for feature, corr in correlations.items():
            if feature == 'Close':
                continue
            if len(selected_features) >= max_features:
                break
            
            # ê¸°ì¡´ íŠ¹ì„±ë“¤ê³¼ì˜ ìƒê´€ê´€ê³„ í™•ì¸
            if len(selected_features) > 2:
                existing_corr = data[selected_features + [feature]].corr()[feature].abs()
                if existing_corr.max() < correlation_threshold:
                    selected_features.append(feature)
            else:
                selected_features.append(feature)
        
        print(f"âœ… ì„ íƒëœ íŠ¹ì„± ({len(selected_features)}ê°œ): {selected_features}")
        return selected_features
    
    def objective(self, trial):
        """Optuna ìµœì í™” ëª©ì  í•¨ìˆ˜"""
        try:
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì œì•ˆ
            lstm_units = trial.suggest_categorical('lstm_units', [16, 32, 64, 128])
            dense_units = trial.suggest_categorical('dense_units', [8, 16, 32, 64])
            dropout_rate = trial.suggest_float('dropout_rate', 0.1, 0.5)
            sequence_length = trial.suggest_categorical('sequence_length', [15, 30, 45, 60])
            batch_size = trial.suggest_categorical('batch_size', [16, 32, 64, 128])
            learning_rate = trial.suggest_float('learning_rate', 1e-4, 1e-2, log=True)
            epochs = trial.suggest_int('epochs', 20, 100)
            
            # íŠ¹ì„± ê°œìˆ˜ ì œì•ˆ
            max_features = trial.suggest_int('max_features', 3, 15)
            
            # ë°ì´í„° ì¤€ë¹„
            data_manager = GoldDataManager('data/gold_data.csv')
            enhanced_data = data_manager.load_or_download_data()
            
            if enhanced_data is None:
                return float('inf')
            
            # ì ì§„ì  íŠ¹ì„± ì„ íƒ
            features = self.get_progressive_features(enhanced_data, max_features)
            
            # ëª¨ë¸ ìƒì„± (ì„ì‹œ ë””ë ‰í† ë¦¬ ì‚¬ìš©)
            model_name = os.path.join(self.tmp_dir, f"temp_model_{trial.number}.h5")
            scaler_name = os.path.join(self.tmp_dir, f"temp_scaler_{trial.number}")
            
            predictor = GoldPredictionModel(
                sequence_length=sequence_length,
                model_name=model_name,
                scaler_name=scaler_name,
                feature_columns=features
            )
            
            # í•˜ì´í¼íŒŒë¼ë¯¸í„° ì„¤ì •
            predictor.lstm_units = [lstm_units]
            predictor.dense_units = [dense_units]
            predictor.dropout_rates = [dropout_rate, dropout_rate * 0.7]
            
            # ë°ì´í„° ì „ì²˜ë¦¬
            result = predictor.prepare_data(enhanced_data, train_ratio=0.8, target_column='Close')
            if result is None:
                return float('inf')
            
            X_train, y_train, X_test, y_test, scaled_data, train_size = result
            
            # ëª¨ë¸ í›ˆë ¨
            predictor.build_model()
            predictor.model.optimizer.learning_rate = learning_rate
            
            # ì¡°ê¸° ì¢…ë£Œ ì„¤ì •
            early_stopping = tf.keras.callbacks.EarlyStopping(
                monitor='val_loss', patience=10, restore_best_weights=True
            )
            
            history = predictor.train(
                X_train, y_train,
                epochs=epochs,
                batch_size=batch_size,
                callbacks=[early_stopping]
            )
            
            # ì„±ëŠ¥ í‰ê°€
            performance = predictor.evaluate(X_train, y_train, X_test, y_test)
            r2_score = performance['test_r2']
            
            # MLflow ë¡œê¹…
            with mlflow.start_run(nested=True):
                mlflow.log_params({
                    'lstm_units': lstm_units,
                    'dense_units': dense_units,
                    'dropout_rate': dropout_rate,
                    'sequence_length': sequence_length,
                    'batch_size': batch_size,
                    'learning_rate': learning_rate,
                    'epochs': epochs,
                    'max_features': max_features,
                    'features': features
                })
                
                mlflow.log_metrics({
                    'test_r2': r2_score,
                    'test_rmse': performance['test_rmse'],
                    'test_mae': performance['test_mae'],
                    'test_mape': performance['test_mape'],
                    'overfitting_ratio': performance['test_rmse'] / performance['train_rmse']
                })
                
                # ëª¨ë¸ ì €ì¥
                mlflow.keras.log_model(predictor.model, "model")

                # ì˜ˆì¸¡ ì°¨íŠ¸ ì €ì¥ ë° ë¡œê¹…
                try:
                    close_prices = enhanced_data['Close']
                    chart_path = predictor.plot_results(close_prices, performance, train_size)
                    os.makedirs('results/auto_tuning', exist_ok=True)
                    trial_chart = f"results/auto_tuning/trial_{trial.number}_results.png"
                    shutil.copy(chart_path, trial_chart)
                    mlflow.log_artifact(trial_chart)
                except Exception as _:
                    pass

                # ë¯¸ë˜ ì˜ˆì¸¡ ì €ì¥ ë° ë¡œê¹…
                try:
                    future_prices, prediction_dates, _ = predictor.predict_future(
                        scaled_data, enhanced_data, days=5
                    )
                    if future_prices is not None:
                        import pandas as _pd
                        df_future = _pd.DataFrame({
                            'date': [d.strftime('%Y-%m-%d') for d in prediction_dates],
                            'predicted_price': future_prices.flatten().tolist()
                        })
                        os.makedirs('results/auto_tuning', exist_ok=True)
                        future_csv = f"results/auto_tuning/trial_{trial.number}_future.csv"
                        df_future.to_csv(future_csv, index=False)
                        mlflow.log_artifact(future_csv)
                except Exception as _:
                    pass
            
            # ì„ì‹œ íŒŒì¼ ì •ë¦¬
            for path in [model_name, scaler_name, f"{scaler_name}_target.pkl", f"{scaler_name}_features.pkl"]:
                try:
                    if os.path.exists(path):
                        os.remove(path)
                except Exception:
                    pass
            
            # ìµœê³  ì„±ëŠ¥ ëª¨ë¸ ì—…ë°ì´íŠ¸
            if r2_score > self.best_r2:
                self.best_r2 = r2_score
                self.best_model = predictor
                self.best_config = {
                    'lstm_units': lstm_units,
                    'dense_units': dense_units,
                    'dropout_rate': dropout_rate,
                    'sequence_length': sequence_length,
                    'batch_size': batch_size,
                    'learning_rate': learning_rate,
                    'epochs': epochs,
                    'max_features': max_features,
                    'features': features
                }
                print(f"ğŸ† ìƒˆë¡œìš´ ìµœê³  ì„±ëŠ¥! RÂ²: {r2_score:.4f}")
            
            return -r2_score  # OptunaëŠ” ìµœì†Œí™”ë¥¼ ëª©í‘œë¡œ í•˜ë¯€ë¡œ ìŒìˆ˜ ë°˜í™˜
            
        except Exception as e:
            print(f"âŒ ì‹¤í—˜ ì‹¤íŒ¨: {str(e)}")
            return float('inf')
    
    def run_auto_tuning(self):
        """ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹ ì‹¤í–‰"""
        print("\nğŸ”¬ Optuna ìë™ íŠœë‹ ì‹œì‘...")
        
        # Optuna ìŠ¤í„°ë”” ìƒì„±
        study = optuna.create_study(
            direction='minimize',
            sampler=optuna.samplers.TPESampler(seed=42)
        )
        
        # ëª©í‘œ ë‹¬ì„± ì‹œ ì¦‰ì‹œ ì¤‘ë‹¨í•˜ëŠ” ì½œë°±
        def _stop_on_target(s, t):
            try:
                best_value = s.best_value  # ìµœì†Œí™”ëœ ê°’ = -best_r2
                if best_value is not None and -best_value >= self.target_r2:
                    s.stop()
            except Exception:
                pass
        
        # ìµœì í™” ì‹¤í–‰
        study.optimize(self.objective, n_trials=self.max_trials, callbacks=[_stop_on_target])
        
        print(f"\nâœ… ìë™ íŠœë‹ ì™„ë£Œ!")
        print(f"ğŸ† ìµœê³  RÂ²: {self.best_r2:.4f}")
        print(f"ğŸ¯ ëª©í‘œ RÂ²: {self.target_r2}")
        
        if self.best_r2 >= self.target_r2:
            print("ğŸ‰ ëª©í‘œ ë‹¬ì„±!")
            return True
        else:
            print("âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„± - ì¶”ê°€ ê°œì„  í•„ìš”")
            return False
    
    def expand_data_period(self, years=2):
        """ë°ì´í„° ê¸°ê°„ í™•ì¥"""
        print(f"\nğŸ“ˆ ë°ì´í„° ê¸°ê°„ í™•ì¥ ì¤‘... (+{years}ë…„)")
        
        data_manager = GoldDataManager('data/gold_data.csv')
        current_data = data_manager.load_or_download_data()
        
        if current_data is not None:
            # ë‚ ì§œëŠ” ì¸ë±ìŠ¤ë¡œ ê´€ë¦¬ë˜ë¯€ë¡œ ì¸ë±ìŠ¤ì—ì„œ ê³„ì‚°
            if isinstance(current_data.index, pd.DatetimeIndex):
                current_start = current_data.index.min()
            else:
                # ì¸ë±ìŠ¤ê°€ ë‚ ì§œê°€ ì•„ë‹ˆë¼ë©´ ë³€í™˜ ì‹œë„
                try:
                    current_data.index = pd.to_datetime(current_data.index)
                    current_start = current_data.index.min()
                except Exception:
                    raise ValueError("ë°ì´í„° ì¸ë±ìŠ¤ë¥¼ ë‚ ì§œí˜•ìœ¼ë¡œ ë³€í™˜í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            new_start = current_start - timedelta(days=years * 365)
            
            print(f"   ê¸°ì¡´ ì‹œì‘ì¼: {current_start}")
            print(f"   ìƒˆë¡œìš´ ì‹œì‘ì¼: {new_start}")
            
            # ìƒˆë¡œìš´ ë°ì´í„° ë‹¤ìš´ë¡œë“œ
            data_manager._download_and_enhance_data(
                start_date=new_start.strftime('%Y-%m-%d'),
                end_date=current_data.index.max().strftime('%Y-%m-%d')
            )
            
            print("âœ… ë°ì´í„° í™•ì¥ ì™„ë£Œ")
            return True
        
        return False
    
    def create_ensemble_model(self, n_models=5):
        """ì•™ìƒë¸” ëª¨ë¸ ìƒì„±"""
        print(f"\nğŸ¤– ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì¤‘... ({n_models}ê°œ ëª¨ë¸)")
        
        ensemble_models = []
        ensemble_configs = []
        
        # ë‹¤ì–‘í•œ ì„¤ì •ìœ¼ë¡œ ì—¬ëŸ¬ ëª¨ë¸ í›ˆë ¨
        for i in range(n_models):
            print(f"   ëª¨ë¸ {i+1}/{n_models} í›ˆë ¨ ì¤‘...")
            
            # ëœë¤ ì„¤ì • ìƒì„±
            config = {
                'lstm_units': np.random.choice([32, 64, 128]),
                'dense_units': np.random.choice([16, 32, 64]),
                'dropout_rate': np.random.uniform(0.2, 0.4),
                'sequence_length': np.random.choice([30, 45, 60]),
                'batch_size': np.random.choice([32, 64]),
                'learning_rate': np.random.uniform(1e-4, 1e-3),
                'epochs': np.random.randint(30, 80),
                'max_features': np.random.randint(5, 12)
            }
            
            # ëª¨ë¸ í›ˆë ¨
            try:
                data_manager = GoldDataManager('data/gold_data.csv')
                enhanced_data = data_manager.load_or_download_data()
                
                if enhanced_data is None:
                    continue
                
                features = self.get_progressive_features(enhanced_data, config['max_features'])
                
                model_name = os.path.join(self.ensemble_dir, f"ensemble_model_{i}.h5")
                scaler_name = os.path.join(self.ensemble_dir, f"ensemble_scaler_{i}")
                
                predictor = GoldPredictionModel(
                    sequence_length=config['sequence_length'],
                    model_name=model_name,
                    scaler_name=scaler_name,
                    feature_columns=features
                )
                
                predictor.lstm_units = [config['lstm_units']]
                predictor.dense_units = [config['dense_units']]
                predictor.dropout_rates = [config['dropout_rate'], config['dropout_rate'] * 0.7]
                
                result = predictor.prepare_data(enhanced_data, train_ratio=0.8, target_column='Close')
                if result is None:
                    continue
                
                X_train, y_train, X_test, y_test, scaled_data, train_size = result
                
                predictor.build_model()
                predictor.model.optimizer.learning_rate = config['learning_rate']
                
                early_stopping = tf.keras.callbacks.EarlyStopping(
                    monitor='val_loss', patience=15, restore_best_weights=True
                )
                
                predictor.train(
                    X_train, y_train,
                    epochs=config['epochs'],
                    batch_size=config['batch_size'],
                    callbacks=[early_stopping]
                )
                
                performance = predictor.evaluate(X_train, y_train, X_test, y_test)
                
                if performance['test_r2'] > 0.3:  # ìµœì†Œ ì„±ëŠ¥ ê¸°ì¤€
                    ensemble_models.append(predictor)
                    ensemble_configs.append(config)
                    print(f"   âœ… ëª¨ë¸ {i+1} ì„±ëŠ¥: RÂ² = {performance['test_r2']:.4f}")
                else:
                    print(f"   âŒ ëª¨ë¸ {i+1} ì„±ëŠ¥ ë¶€ì¡±: RÂ² = {performance['test_r2']:.4f}")
                
            except Exception as e:
                print(f"   âŒ ëª¨ë¸ {i+1} ì‹¤íŒ¨: {str(e)}")
                continue
        
        if len(ensemble_models) >= 2:
            print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ {len(ensemble_models)}ê°œ ìƒì„± ì™„ë£Œ")
            return ensemble_models, ensemble_configs
        else:
            print("âŒ ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì‹¤íŒ¨")
            return None, None
    
    def run_complete_optimization(self):
        """ì™„ì „ ìë™í™”ëœ ìµœì í™” ì‹¤í–‰"""
        print("ğŸš€ ì™„ì „ ìë™í™”ëœ ê¸ˆ ê°€ê²© ì˜ˆì¸¡ ìµœì í™” ì‹œì‘!")
        print("="*60)
        
        # 1ë‹¨ê³„: ê¸°ë³¸ ìë™ íŠœë‹
        print("\nğŸ“Š 1ë‹¨ê³„: ìë™ í•˜ì´í¼íŒŒë¼ë¯¸í„° íŠœë‹")
        success = self.run_auto_tuning()
        
        if success:
            print("ğŸ‰ 1ë‹¨ê³„ì—ì„œ ëª©í‘œ ë‹¬ì„±!")
            return self.best_model, self.best_config
        
        # 2ë‹¨ê³„: ë°ì´í„° í™•ì¥
        print("\nğŸ“ˆ 2ë‹¨ê³„: ë°ì´í„° ê¸°ê°„ í™•ì¥")
        if self.expand_data_period(years=2):
            print("ğŸ”„ í™•ì¥ëœ ë°ì´í„°ë¡œ ì¬íŠœë‹...")
            success = self.run_auto_tuning()
            
            if success:
                print("ğŸ‰ 2ë‹¨ê³„ì—ì„œ ëª©í‘œ ë‹¬ì„±!")
                return self.best_model, self.best_config
        
        # 3ë‹¨ê³„: ì•™ìƒë¸” ëª¨ë¸
        print("\nğŸ¤– 3ë‹¨ê³„: ì•™ìƒë¸” ëª¨ë¸ ìƒì„±")
        ensemble_models, ensemble_configs = self.create_ensemble_model(n_models=5)
        
        if ensemble_models:
            print("âœ… ì•™ìƒë¸” ëª¨ë¸ ìƒì„± ì™„ë£Œ")
            return ensemble_models, ensemble_configs
        
        print("âŒ ëª¨ë“  ë‹¨ê³„ì—ì„œ ëª©í‘œ ë‹¬ì„± ì‹¤íŒ¨")
        return self.best_model, self.best_config

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    print("ğŸ† ìë™í™”ëœ ê¸ˆ ê°€ê²© ì˜ˆì¸¡ ì‹œìŠ¤í…œ")
    print("="*50)
    
    # CLI ì¸ì íŒŒì„œ
    parser = argparse.ArgumentParser(description='Gold price auto-tuning system')
    parser.add_argument('--target-r2', type=float, default=None, help='ëª©í‘œ RÂ² (ì˜ˆ: 0.5)')
    parser.add_argument('--max-trials', type=int, default=None, help='ìµœëŒ€ ì‹¤í—˜ íšŸìˆ˜ (ì˜ˆ: 50)')
    args, unknown = parser.parse_known_args()

    # ëª©í‘œ RÂ² ì„¤ì • (CLI > ENV > interactive)
    target_r2 = args.target_r2
    if target_r2 is None:
        target_r2 = float(os.environ.get('NAUTILUS_TARGET_R2') or input("ëª©í‘œ RÂ² ì ìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 0.5): ") or "0.5")

    # ìµœëŒ€ ì‹¤í—˜ íšŸìˆ˜ ì„¤ì • (CLI > ENV > interactive)
    max_trials = args.max_trials
    if max_trials is None:
        max_trials = int(os.environ.get('NAUTILUS_MAX_TRIALS') or input("ìµœëŒ€ ì‹¤í—˜ íšŸìˆ˜ë¥¼ ì…ë ¥í•˜ì„¸ìš” (ê¸°ë³¸ê°’: 50): ") or "50")
    
    # ìë™ íŠœë‹ ì‹œìŠ¤í…œ ì‹œì‘
    auto_system = AutoTuningSystem(target_r2=target_r2, max_trials=max_trials)
    
    # ì™„ì „ ìë™í™”ëœ ìµœì í™” ì‹¤í–‰
    best_model, best_config = auto_system.run_complete_optimization()
    
    print(f"\nğŸ† ìµœì¢… ê²°ê³¼:")
    print(f"   ìµœê³  RÂ²: {auto_system.best_r2:.4f}")
    print(f"   ëª©í‘œ RÂ²: {target_r2}")
    
    if auto_system.best_r2 >= target_r2:
        print("ğŸ‰ ëª©í‘œ ë‹¬ì„±!")
    else:
        print("âš ï¸ ëª©í‘œ ë¯¸ë‹¬ì„± - ì¶”ê°€ ê°œì„  í•„ìš”")

if __name__ == "__main__":
    main()
